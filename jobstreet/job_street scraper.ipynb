{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver import Chrome\n",
    "import pandas as pd\n",
    "import re \n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_number(keyword):\n",
    "    #input: keyword for job_postings\n",
    "    #output: number of pages\n",
    "\n",
    "    url = base_url.format(keyword, 1)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    result_text = soup.find(\"span\",{\"class\": \"sx2jih0 zcydq84u _18qlyvc0 _18qlyvc1x _18qlyvc1 _1d0g9qk4 _18qlyvc8\"})\n",
    "    results = result_text.text.split()\n",
    "    print(results)\n",
    "    result = int(result_text.text.split()[-2].replace(',', ''))\n",
    "    page_number = math.ceil(result/30)\n",
    "    \n",
    "    return page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_page_scraper(link):\n",
    "\n",
    "    url = \"https://www.jobstreet.com.sg\"+link\n",
    "#     print(\"scraping...\", url)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    scripts = soup.find_all(\"script\")\n",
    "\n",
    "    for script in scripts:\n",
    "        if script.contents:\n",
    "            txt = script.contents[0].strip()\n",
    "            if 'window.REDUX_STATE = ' in txt:\n",
    "                jsonStr = script.contents[0].strip()\n",
    "                jsonStr = jsonStr.split('window.REDUX_STATE = ')[1].strip()\n",
    "                jsonStr = jsonStr.split('}}}};')[0].strip()\n",
    "                jsonStr = jsonStr+\"}}}}\"\n",
    "                jsonObj = json.loads(jsonStr)\n",
    "                \n",
    "    job = jsonObj['details']\n",
    "    job_id = job['id']\n",
    "    job_expired = job['isExpired']\n",
    "    job_confidential = job['isConfidential']\n",
    "    job_salary_min = job['header']['salary']['min']\n",
    "    job_salary_max = job['header']['salary']['max']\n",
    "    job_salary_currency = job['header']['salary']['currency']\n",
    "    job_title = job['header']['jobTitle']\n",
    "    company = job['header']['company']['name']\n",
    "    job_post_date = job['header']['postedDate']\n",
    "    job_internship = job['header']['isInternship']\n",
    "    company_website = job['companyDetail']['companyWebsite']\n",
    "    company_avgProcessTime = job['companyDetail']['companySnapshot']['avgProcessTime']\n",
    "    company_registrationNo = job['companyDetail']['companySnapshot']['registrationNo']\n",
    "    company_workingHours = job['companyDetail']['companySnapshot']['workingHours']\n",
    "    company_facebook = job['companyDetail']['companySnapshot']['facebook']\n",
    "    company_size = job['companyDetail']['companySnapshot']['size']\n",
    "    company_dressCode = job['companyDetail']['companySnapshot']['dressCode']\n",
    "    company_nearbyLocations = job['companyDetail']['companySnapshot']['nearbyLocations']\n",
    "    company_overview = job['companyDetail']['companyOverview']['html']\n",
    "    job_description = job['jobDetail']['jobDescription']['html']\n",
    "    job_summary = job['jobDetail']['summary']\n",
    "    job_requirement_career_level = job['jobDetail']['jobRequirement']['careerLevel']\n",
    "    job_requirement_yearsOfExperience = job['jobDetail']['jobRequirement']['yearsOfExperience']\n",
    "    job_requirement_qualification = job['jobDetail']['jobRequirement']['qualification']\n",
    "    job_requirement_fieldOfStudy = job['jobDetail']['jobRequirement']['fieldOfStudy']\n",
    "    #job_requirement_industry = job['jobDetail']['jobRequirement']['industryValue']['label']\n",
    "    job_requirement_skill = job['jobDetail']['jobRequirement']['skills']\n",
    "    job_employment_type = job['jobDetail']['jobRequirement']['employmentType']\n",
    "    job_languages = job['jobDetail']['jobRequirement']['languages']\n",
    "    job_benefits = job['jobDetail']['jobRequirement']['benefits']\n",
    "    job_apply_url = job['applyUrl']['url']\n",
    "    job_location_zipcode = job['location'][0]['locationId']\n",
    "    job_location = job['location'][0]['location']\n",
    "    job_country = job['sourceCountry']\n",
    "\n",
    "    return [job_id, job_title, job_expired, job_confidential, job_salary_min, job_salary_max, job_salary_currency, company, job_post_date, job_internship, company_website, company_avgProcessTime, company_registrationNo, company_workingHours, company_facebook, company_size, company_dressCode, company_nearbyLocations, company_overview, job_description, job_summary, job_requirement_career_level, job_requirement_fieldOfStudy, job_requirement_yearsOfExperience, job_requirement_qualification, job_requirement_skill, job_employment_type, job_languages, job_benefits, job_apply_url, job_location_zipcode, job_location, job_country]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_crawler(keyword, numOfPage=1):\n",
    "    # input: keyword for job postings\n",
    "    # output: dataframe of links scraped from each page\n",
    "\n",
    "    # page number\n",
    "#     page_number = get_page_number(keyword)\n",
    "    job_links = []\n",
    "\n",
    "#     for n in range(page_number):\n",
    "    for n in range(numOfPage):\n",
    "        print('Loading page {} ...'.format(n+1))\n",
    "        url = base_url.format(keyword, n+1)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "        #extract all job links\n",
    "        links = soup.find_all('a',{'class':'_1hr6tkx5'})\n",
    "        job_links += links\n",
    " \n",
    "    jobs = []\n",
    "    num_of_jobs = len(job_links)\n",
    "    print(\"scraping\",num_of_jobs,\"jobs...\")\n",
    "    bar = progressbar.ProgressBar(maxval=num_of_jobs, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    for index, link in enumerate(job_links):\n",
    "        bar.update(index)\n",
    "        job_link = link['href'].strip().split('?', 1)[0]\n",
    "        if \"jobs\" not in job_link:\n",
    "            try:\n",
    "                jobs.append([keyword, job_link] + job_page_scraper(job_link))\n",
    "            except Exception as e:\n",
    "                print(\"Error occured\", e)\n",
    "    bar.finish()\n",
    "    result_df = pd.DataFrame(jobs, columns = ['keyword', 'link', 'job_id', 'job_title', 'job_expired', 'job_confidential', 'job_salary_min', 'job_salary_max', 'job_salary_currency', 'company', 'job_post_date', 'job_internship', 'company_website', 'company_avgProcessTime', 'company_registrationNo', 'company_workingHours', 'company_facebook', 'company_size', 'company_dressCode', 'company_nearbyLocations', 'company_overview', 'job_description', 'job_summary', 'job_requirement_career_level', 'job_requirement_fieldOfStudy', 'job_requirement_yearsOfExperience', 'job_requirement_qualification', 'job_requirement_skill', 'job_employment_type', 'job_languages', 'job_benefits', 'job_apply_url', 'job_location_zipcode', 'job_location', 'job_country'])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "path = \"C:/Users/teo-e/OneDrive/Desktop/SchoolWork/IS434 SA/project/chromedriver.exe\"\n",
    "driver = Chrome(executable_path=path)\n",
    "time.sleep(2)\n",
    "base_url = \"https://www.jobstreet.com.sg/en/job-search/{}-jobs/{}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----scraping uiux designer ----\n",
      "Loading page 1 ...\n",
      "Loading page 2 ...\n",
      "Loading page 3 ...\n",
      "Loading page 4 ...\n",
      "Loading page 5 ...\n",
      "Loading page 6 ...\n",
      "Loading page 7 ...\n",
      "Loading page 8 ...\n",
      "Loading page 9 ...\n",
      "Loading page 10 ...\n",
      "Loading page 11 ...\n",
      "Loading page 12 ...\n",
      "Loading page 13 ...\n",
      "Loading page 14 ...\n",
      "Loading page 15 ...\n",
      "Loading page 16 ...\n",
      "Loading page 17 ...\n",
      "Loading page 18 ...\n",
      "Loading page 19 ...\n",
      "Loading page 20 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 2599 jobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[==                                                                      ]   3%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured local variable 'jsonObj' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========                                                                ]  12%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured local variable 'jsonObj' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[===========================================                             ]  61%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured local variable 'jsonObj' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[============================================                            ]  62%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured local variable 'jsonObj' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[===========================================================             ]  82%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured local variable 'jsonObj' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----done scraping uiux designer -----\n",
      "----scraping tech project manager ----\n",
      "Loading page 1 ...\n",
      "Loading page 2 ...\n",
      "Loading page 3 ...\n",
      "Loading page 4 ...\n",
      "Loading page 5 ...\n",
      "Loading page 6 ...\n",
      "Loading page 7 ...\n",
      "Loading page 8 ...\n",
      "Loading page 9 ...\n",
      "Loading page 10 ...\n",
      "Loading page 11 ...\n",
      "Loading page 12 ...\n",
      "Loading page 13 ...\n",
      "Loading page 14 ...\n",
      "Loading page 15 ...\n",
      "Loading page 16 ...\n",
      "Loading page 17 ...\n",
      "Loading page 18 ...\n",
      "Loading page 19 ...\n",
      "Loading page 20 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 2516 jobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=============================                                           ]  40%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured local variable 'jsonObj' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[===============================================================         ]  88%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured local variable 'jsonObj' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----done scraping tech project manager -----\n",
      "---scraping done---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # a list of job roles to be crawled\n",
    "    key_words = ['uiux designer', 'tech project manager']\n",
    "#     dfs = []\n",
    "\n",
    "    for key in key_words:\n",
    "        print(\"----scraping\",key,\"----\")\n",
    "        key_df = page_crawler(key, numOfPage=20)\n",
    "#         dfs.append(key_df)\n",
    "        key_df.to_csv(key+\"_job_postings_results.csv\")\n",
    "        print(\"----done scraping\",key,\"-----\")\n",
    "    \n",
    "    # save scraped information as csv\n",
    "#     pd.concat(dfs).to_csv(\"job_postings_results.csv\")\n",
    "    print(\"---scraping done---\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
