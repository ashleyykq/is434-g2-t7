{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://forums.hardwarezone.com.sg/threads/smus-rebranded-it-school-will-have-greater-focus-on-computing-and-coding.6448391/\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=99.0.4844.74)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8b93320652fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mtitle\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \"\"\"\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_TITLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=99.0.4844.74)\n"
     ]
    }
   ],
   "source": [
    "driver.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thread Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title = driver.find_element_by_class_name(\"p-title-value\").text\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextButton = driver.find_element_by_class_name(\"pageNavSimple-el.pageNavSimple-el--next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextButton.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = driver.find_elements_by_class_name(\"message.message--post.js-post.js-inlineModContainer\")\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[i].find_element_by_class_name(\"message-name\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[i].find_element_by_class_name(\"u-dt\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = articles[i].find_element_by_class_name(\"bbWrapper\").text\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = articles[i].find_element_by_class_name(\"bbCodeBlock.bbCodeBlock--expandable.bbCodeBlock--quote.js-expandWatch\").text\n",
    "test2[len(linked_post)+4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1[len(test2)+2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://forums.hardwarezone.com.sg/threads/smus-rebranded-it-school-will-have-greater-focus-on-computing-and-coding.6448391/\",\n",
    "    \"https://forums.hardwarezone.com.sg/threads/smu-is.4202272/\",\n",
    "    \"https://forums.hardwarezone.com.sg/threads/smu-is-or-smu-biz.5064652/\",\n",
    "    \"https://forums.hardwarezone.com.sg/threads/smu-sis-or-sit-information-communications-technology.5801729/\",\n",
    "    \"https://forums.hardwarezone.com.sg/threads/information-systems-in-smu-vs-nus.5814561/\",\n",
    "    \"https://forums.hardwarezone.com.sg/threads/smu-sis-or-sit-software-engineering.5844900/\",\n",
    "    \"https://forums.hardwarezone.com.sg/threads/is-smu-information-systems-degree-widely-recognized-for-a-better-future.5605977/\",\n",
    "    \"https://forums.hardwarezone.com.sg/threads/smu-information-systems-interview.2713965/\"\n",
    "]\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(urls[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = driver.find_element_by_class_name(\"p-title-value\").text\n",
    "articles = driver.find_elements_by_class_name(\"message.message--post.js-post.js-inlineModContainer\")\n",
    "for i in range(len(articles)):\n",
    "    user = articles[i].find_element_by_class_name(\"message-name\").text\n",
    "    date = articles[i].find_element_by_class_name(\"u-dt\").text\n",
    "    thread_post = articles[i].find_element_by_class_name(\"bbWrapper\").text\n",
    "    try:\n",
    "        articles[i].find_element_by_class_name(\"bbCodeBlock.bbCodeBlock--expandable.bbCodeBlock--quote.js-expandWatch\")\n",
    "        linked_post = articles[i].find_element_by_class_name(\"bbCodeBlock.bbCodeBlock--expandable.bbCodeBlock--quote.js-expandWatch\").text\n",
    "        thread_post = thread_post[len(linked_post)+2:]\n",
    "    except:\n",
    "        linked_post = \"-\"\n",
    "    scraped_data.append([title, user,date,linked_post,thread_post])\n",
    "try:\n",
    "    driver.find_element_by_class_name(\"pageNavSimple-el.pageNavSimple-el--next\")\n",
    "    nextButton = driver.find_element_by_class_name(\"pageNavSimple-el.pageNavSimple-el--next\")\n",
    "except:\n",
    "    nextButton = False\n",
    "while(nextButton):\n",
    "    nextButton.click()\n",
    "    WebDriverWait(driver,100).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME,\"message.message--post.js-post.js-inlineModContainer\"))\n",
    "    )\n",
    "    articles = driver.find_elements_by_class_name(\"message.message--post.js-post.js-inlineModContainer\")\n",
    "    for i in range(len(articles)):\n",
    "        user = articles[i].find_element_by_class_name(\"message-name\").text\n",
    "        date = articles[i].find_element_by_class_name(\"u-dt\").text\n",
    "        thread_post = articles[i].find_element_by_class_name(\"bbWrapper\").text\n",
    "        try:\n",
    "            articles[i].find_element_by_class_name(\"bbCodeBlock.bbCodeBlock--expandable.bbCodeBlock--quote.js-expandWatch\")\n",
    "            linked_post = articles[i].find_element_by_class_name(\"bbCodeBlock.bbCodeBlock--expandable.bbCodeBlock--quote.js-expandWatch\").text\n",
    "            thread_post = thread_post[len(linked_post)+2:]\n",
    "        except:\n",
    "            linked_post = \"-\"\n",
    "        scraped_data.append([title, user,date,linked_post,thread_post])\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"pageNavSimple-el.pageNavSimple-el--next\")\n",
    "        nextButton = driver.find_element_by_class_name(\"pageNavSimple-el.pageNavSimple-el--next\")\n",
    "    except:\n",
    "        nextButton = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"HardwareZone_SIS.xlsx\"\n",
    "f = open(filename,\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_file = open(filename,'r')\n",
    "check_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data = scraped_data, columns =[\"Thread Title\",\"Username\",\"post date\",\"linked post\",\"thread post\"])\n",
    "# df.to_excel(\"HardwareZone_SIS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './is434-g2-t7/HardwareZone_SIS.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-12266af1a441>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./is434-g2-t7/HardwareZone_SIS.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Unnamed: 0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1377\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m                 )\n",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m   1251\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m     ) as handle:\n",
      "\u001b[1;32mC:\\Anaconda_File\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './is434-g2-t7/HardwareZone_SIS.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# nltk - natural langauge processing\n",
    "# wordcloud - for drawing word cloud\n",
    "# matplotlib - for charting\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel(\"./is434-g2-t7/HardwareZone_SIS.xlsx\")\n",
    "df.drop(columns = \"Unnamed: 0\", inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.append(df2, ignore_index = True)\n",
    "df3.to_excel(\"HardwareZone_SIS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, '', regex=True)\n",
    "df = df.replace(r'[-]+', ' ', regex=True)\n",
    "df = df.replace('NIL|nil|Nil', '', regex=True)\n",
    "df = df.replace(\"\\n\\n\",\". \", regex = True)\n",
    "df\n",
    "# df3 = df3.replace(np.nan, '', regex=True)\n",
    "# df3 = df3.replace(r'[-]+', ' ', regex=True)\n",
    "# df3 = df3.replace('NIL|nil|Nil', '', regex=True)\n",
    "# df3 = df3.replace(\"\\n\\n\",\". \", regex = True)\n",
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()\n",
    "df4\n",
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "df4['thread_post_processed'] = df4['thread post'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "df4['thread_post_processed'] = df4['thread_post_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "df4['thread_post_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df.copy()\n",
    "df5\n",
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "df5['thread_post_processed'] = df5['thread post'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "df5['thread_post_processed'] = df5['thread_post_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "df5['thread_post_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['congrats', 'lol', 'yup', 'thanks', 'nope', \n",
    "                       'http', 'www', 'com', 'https','amp', 'sg', \n",
    "                       'reddit', 'gt', 'la','lor', 'le', 'leh', 'smu', 'ntu' , 'nus'])\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "\n",
    "for (columnName, columnData) in df4.iteritems():\n",
    "    qn_content = ''\n",
    "    for line in df5[columnName]:\n",
    "        if len(line.strip()) > 0: # Eliminates empty answers\n",
    "            qn_content += line + ' '    \n",
    "    \n",
    "    # Tokenize Words\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words_content = tokenizer.tokenize(qn_content)  # All answers for that question into words\n",
    "    #print(words_content)\n",
    "    #print(len(words_content))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words_filtered = []\n",
    "    for w in words_content:\n",
    "        if w not in stop_words:\n",
    "            words_filtered.append(w)\n",
    "            \n",
    "    #print(words_filtered)\n",
    "    #print(len(words_filtered))\n",
    "    \n",
    "    # Porter Stemmer\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    words_stemmed = []\n",
    "    for w in words_filtered:\n",
    "        words_stemmed.append(porter_stemmer.stem(w))\n",
    "        \n",
    "    #print(words_stemmed)\n",
    "    \n",
    "    # WordCloud\n",
    "    words_joined = \" \".join([w for w in words_stemmed])\n",
    "\n",
    "    # Create a word cloud\n",
    "    my_wordcloud = WordCloud(background_color='white',\n",
    "                         width=1800,\n",
    "                         height=1400).generate(words_joined)\n",
    "\n",
    "    plt.imshow(my_wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.title(columnName)\n",
    "    plt.show()\n",
    "    # plt.savefig(columnName, dpi=300)\n",
    "    # Initialise the count vectorizer with the English stop words\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the processed titles\n",
    "    count_data = count_vectorizer.fit_transform(df5[columnName])\n",
    "\n",
    "    # Visualise the 10 most common words\n",
    "    plot_10_most_common_words(count_data, count_vectorizer)\n",
    "    \n",
    "    # Tweak the two parameters below (use int values below 15)\n",
    "    number_topics = 5\n",
    "    number_words = 10\n",
    "\n",
    "    # Create and fit the LDA model\n",
    "    lda = LDA(n_components=number_topics)\n",
    "    lda.fit(count_data)\n",
    "\n",
    "    # Print the topics found by the LDA model\n",
    "    print(\"Topics found via LDA:\")\n",
    "    print_topics(lda, count_vectorizer, number_words)\n",
    "    \n",
    "\n",
    "#     from pyLDAvis import sklearn as sklearn_lda\n",
    "#     import pickle \n",
    "#     import pyLDAvis\n",
    "\n",
    "#     # Visualize the topics\n",
    "#     pyLDAvis.enable_notebook()\n",
    "\n",
    "#     LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "#     # # this is a bit time consuming - make the if statement True\n",
    "#     # # if you want to execute visualization prep yourself\n",
    "#     if 1 == 1:\n",
    "\n",
    "#         LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "#         with open(LDAvis_data_filepath, 'w') as f:\n",
    "#             pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "#     # load the pre-prepared pyLDAvis data from disk\n",
    "#     with open(LDAvis_data_filepath) as f:\n",
    "#         LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "#     pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')\n",
    "\n",
    "#     LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['congrats', 'lol', 'yup', 'thanks', 'nope', \n",
    "                       'http', 'www', 'com', 'https','amp', 'sg', \n",
    "                       'reddit', 'gt', 'la','lor', 'le', 'leh', 'smu', 'ntu' , 'nus'])\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "\n",
    "for (columnName, columnData) in df4.iteritems():\n",
    "    qn_content = ''\n",
    "    for line in df5[columnName]:\n",
    "        if len(line.strip()) > 0: # Eliminates empty answers\n",
    "            qn_content += line + ' '    \n",
    "    \n",
    "    # Tokenize Words\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words_content = tokenizer.tokenize(qn_content)  # All answers for that question into words\n",
    "    #print(words_content)\n",
    "    #print(len(words_content))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words_filtered = []\n",
    "    for w in words_content:\n",
    "        if w not in stop_words:\n",
    "            words_filtered.append(w)\n",
    "            \n",
    "    #print(words_filtered)\n",
    "    #print(len(words_filtered))\n",
    "    \n",
    "    # Porter Stemmer\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    words_stemmed = []\n",
    "    for w in words_filtered:\n",
    "        words_stemmed.append(porter_stemmer.stem(w))\n",
    "        \n",
    "    #print(words_stemmed)\n",
    "    \n",
    "    # WordCloud\n",
    "    words_joined = \" \".join([w for w in words_stemmed])\n",
    "\n",
    "    # Create a word cloud\n",
    "    my_wordcloud = WordCloud(background_color='white',\n",
    "                         width=1800,\n",
    "                         height=1400).generate(words_joined)\n",
    "\n",
    "    plt.imshow(my_wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.title(columnName)\n",
    "    plt.show()\n",
    "    # plt.savefig(columnName, dpi=300)\n",
    "    # Initialise the count vectorizer with the English stop words\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the processed titles\n",
    "    count_data = count_vectorizer.fit_transform(df5[columnName])\n",
    "\n",
    "    # Visualise the 10 most common words\n",
    "    plot_10_most_common_words(count_data, count_vectorizer)\n",
    "    \n",
    "    # Tweak the two parameters below (use int values below 15)\n",
    "    number_topics = 5\n",
    "    number_words = 10\n",
    "\n",
    "    # Create and fit the LDA model\n",
    "    lda = LDA(n_components=number_topics)\n",
    "    lda.fit(count_data)\n",
    "\n",
    "    # Print the topics found by the LDA model\n",
    "    print(\"Topics found via LDA:\")\n",
    "    print_topics(lda, count_vectorizer, number_words)\n",
    "    \n",
    "\n",
    "#     from pyLDAvis import sklearn as sklearn_lda\n",
    "#     import pickle \n",
    "#     import pyLDAvis\n",
    "\n",
    "#     # Visualize the topics\n",
    "#     pyLDAvis.enable_notebook()\n",
    "\n",
    "#     LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "#     # # this is a bit time consuming - make the if statement True\n",
    "#     # # if you want to execute visualization prep yourself\n",
    "#     if 1 == 1:\n",
    "\n",
    "#         LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "#         with open(LDAvis_data_filepath, 'w') as f:\n",
    "#             pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "#     # load the pre-prepared pyLDAvis data from disk\n",
    "#     with open(LDAvis_data_filepath) as f:\n",
    "#         LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "#     pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')\n",
    "\n",
    "#     LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qn_content = ''\n",
    "for line in df4[\"thread_post_processed\"]:\n",
    "    if len(line.strip()) > 0: # Eliminates empty answers\n",
    "        qn_content += line + ' '    \n",
    "\n",
    "# Tokenize Words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_content = tokenizer.tokenize(qn_content)  # All answers for that question into words\n",
    "#print(words_content)\n",
    "#print(len(words_content))\n",
    "\n",
    "# Remove stop words\n",
    "words_filtered = []\n",
    "for w in words_content:\n",
    "    if w not in stop_words:\n",
    "        words_filtered.append(w)\n",
    "\n",
    "#print(words_filtered)\n",
    "#print(len(words_filtered))\n",
    "\n",
    "# Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "words_stemmed = []\n",
    "for w in words_filtered:\n",
    "    words_stemmed.append(porter_stemmer.stem(w))\n",
    "\n",
    "#print(words_stemmed)\n",
    "\n",
    "# WordCloud\n",
    "words_joined = \" \".join([w for w in words_stemmed])\n",
    "\n",
    "# Create a word cloud\n",
    "my_wordcloud = WordCloud(background_color='white',\n",
    "                     width=1800,\n",
    "                     height=1400).generate(words_joined)\n",
    "\n",
    "plt.imshow(my_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title(columnName)\n",
    "plt.show()\n",
    "# plt.savefig(columnName, dpi=300)\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(df5[\"thread_post_processed\"])\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(count_data, count_vectorizer)\n",
    "\n",
    "# Tweak the two parameters below (use int values below 15)\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)\n",
    "\n",
    "\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # # this is a bit time consuming - make the if statement True\n",
    "# # # if you want to execute visualization prep yourself\n",
    "# if 1 == 1:\n",
    "\n",
    "#     LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "#     with open(LDAvis_data_filepath, 'w') as f:\n",
    "#         pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# # load the pre-prepared pyLDAvis data from disk\n",
    "# with open(LDAvis_data_filepath) as f:\n",
    "#     LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "# pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')\n",
    "\n",
    "# LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-226737562063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HardwareZone_SIS.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df4' is not defined"
     ]
    }
   ],
   "source": [
    "df4.to_csv(\"HardwareZone_SIS.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
